{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc=SparkContext()\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = spark.sparkContext.sequenceFile('s3://datasets.elasticmapreduce/ngrams/books/20090715/eng-gb-all/1gram/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os,datetime, re\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tp(tp):\n",
    "    (index, line) = tp\n",
    "    tri_gram, y, n1, n2, n3 = re.split(r'[\\t]', line)\n",
    "    words = tri_gram.split()\n",
    "    while len(words)<3:\n",
    "        words.append('Null')   \n",
    "    match_count=int(n1)\n",
    "    return (words[0],words[1],words[2]), match_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gram = trigram.map(lambda ls: split_tp(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('!', '!', 'That'), 1),\n",
       " (('!', '!', 'That'), 2),\n",
       " (('!', '!', 'That'), 2),\n",
       " (('!', '!', 'That'), 1),\n",
       " (('!', '!', 'That'), 1),\n",
       " (('!', '!', 'That'), 1),\n",
       " (('!', '!', 'That'), 6),\n",
       " (('!', '!', 'That'), 1),\n",
       " (('!', '!', 'That'), 1),\n",
       " (('!', '!', 'That'), 3)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_gram.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a,b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_rdd = new_gram.reduceByKey(add).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('\".', 'Arensky', 'Null'), 44),\n",
       " (('.', 'Assumes', 'a'), 211),\n",
       " (('.', 'At', 'Almaden'), 63),\n",
       " (('.', 'At', 'Avranches'), 62),\n",
       " (('.', 'Buses', 'Terminal'), 581),\n",
       " (('.', 'But', 'obstacles'), 156),\n",
       " (('.', 'But', 'patriotism'), 249),\n",
       " (('.', 'By', 'caste'), 43),\n",
       " (('.', 'CORROSIVE', 'SUBLIMATE'), 95),\n",
       " (('.', 'Can', 'work'), 94),\n",
       " (('.', 'Carrying', 'all'), 97),\n",
       " (('.', 'Catharina', 'Alexowna'), 52),\n",
       " (('.', 'Cavalry', 'was'), 343),\n",
       " (('.', 'Chancellor', 'Adenauer'), 100),\n",
       " (('.', 'Chief', 'in'), 246),\n",
       " (('.', 'Climatic', 'Conditions'), 79),\n",
       " (('\".', 'Cobbctt', 'Null'), 78),\n",
       " (('.', 'Common', 'rules'), 60),\n",
       " (('.', 'Comprising', 'also'), 406),\n",
       " (('.', 'Desiccating', '—'), 82)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_rdd.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rowtransfer(element):\n",
    "    words, value = element\n",
    "    wd1, wd2, wd3 = words\n",
    "    return Row(word_12= wd1+' '+wd2, word_3=wd3, \n",
    "               count=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = reduced_rdd.map(Rowtransfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count=44, word_12='\". Arensky', word_3='Null'),\n",
       " Row(count=211, word_12='. Assumes', word_3='a'),\n",
       " Row(count=63, word_12='. At', word_3='Almaden'),\n",
       " Row(count=62, word_12='. At', word_3='Avranches'),\n",
       " Row(count=581, word_12='. Buses', word_3='Terminal'),\n",
       " Row(count=156, word_12='. But', word_3='obstacles'),\n",
       " Row(count=249, word_12='. But', word_3='patriotism'),\n",
       " Row(count=43, word_12='. By', word_3='caste'),\n",
       " Row(count=95, word_12='. CORROSIVE', word_3='SUBLIMATE'),\n",
       " Row(count=94, word_12='. Can', word_3='work')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_df = spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+----------+\n",
      "|count|      word_12|    word_3|\n",
      "+-----+-------------+----------+\n",
      "|   44|   \". Arensky|      Null|\n",
      "|  211|    . Assumes|         a|\n",
      "|   63|         . At|   Almaden|\n",
      "|   62|         . At| Avranches|\n",
      "|  581|      . Buses|  Terminal|\n",
      "|  156|        . But| obstacles|\n",
      "|  249|        . But|patriotism|\n",
      "|   43|         . By|     caste|\n",
      "|   95|  . CORROSIVE| SUBLIMATE|\n",
      "|   94|        . Can|      work|\n",
      "|   97|   . Carrying|       all|\n",
      "|   52|  . Catharina|  Alexowna|\n",
      "|  343|    . Cavalry|       was|\n",
      "|  100| . Chancellor|  Adenauer|\n",
      "|  246|      . Chief|        in|\n",
      "|   79|   . Climatic|Conditions|\n",
      "|   78|   \". Cobbctt|      Null|\n",
      "|   60|     . Common|     rules|\n",
      "|  406| . Comprising|      also|\n",
      "|   82|. Desiccating|         —|\n",
      "+-----+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wd_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql = wd_df.createOrReplaceTempView('df_sql')\n",
    "df_sql = spark.sql(\"CACHE TABLE df_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------------+\n",
      "|word_12|word_3|         probability|\n",
      "+-------+------+--------------------+\n",
      "|    ! !|     !|  0.3469251002630342|\n",
      "|    ! !|     '| 0.06572854471081241|\n",
      "|    ! !|     .| 0.06372828467700802|\n",
      "|    ! !|     )|  0.0459259703761489|\n",
      "|    ! !|     I| 0.03392441017332253|\n",
      "|    ! !|   The|0.028523708082050667|\n",
      "|    ! !|     (|0.022542930580975527|\n",
      "|    ! !|   and| 0.01760228829747867|\n",
      "|    ! !|    of|0.014631902147279146|\n",
      "|    ! !|   the| 0.01145148869353016|\n",
      "|    ! !|    It|0.010401352175782851|\n",
      "|    ! !|   And|0.009621250762599138|\n",
      "|    ! !|   But|0.008701131147049117|\n",
      "|    ! !|  This|0.008301079140288237|\n",
      "|    ! !|     A|0.008131057037414863|\n",
      "|    ! !|     -|0.007520977727104523|\n",
      "|    ! !|    We|0.006870893216118095|\n",
      "|    ! !|    He|0.006850890615780051|\n",
      "|    ! !|     *|0.006580855511216458|\n",
      "|    ! !|    In|0.006570854211047437|\n",
      "+-------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_count = spark.sql('with cte as\\\n",
    "                         (select word_12,word_3,count,sum(count) over (partition by word_12) as total_count from df_sql)\\\n",
    "                     select word_12,word_3, count/total_count as probability from cte\\\n",
    "                     order by word_12,probability desc')\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_count.createOrReplaceTempView('model')\n",
    "df_model = spark.sql(\"CACHE TABLE model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.sql('''select * from model where word_12 = 'I am'\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+\n",
      "|word_12|   word_3|         probability|\n",
      "+-------+---------+--------------------+\n",
      "|   I am|      not| 0.09659228248056738|\n",
      "|   I am|     sure| 0.04816873417215208|\n",
      "|   I am|        a| 0.03582667933025214|\n",
      "|   I am|   afraid|0.021979895643062568|\n",
      "|   I am|     very|0.021506266043526238|\n",
      "|   I am|      the|0.021107614072265572|\n",
      "|   I am|       in|0.019769075179189473|\n",
      "|   I am|    going|0.019073257691677246|\n",
      "|   I am|     glad| 0.01893294913834549|\n",
      "|   I am|    sorry|0.017583116543939164|\n",
      "|   I am|      now|0.017302421008794186|\n",
      "|   I am|       to|0.016348024817908674|\n",
      "|   I am|        .|0.012685493162212845|\n",
      "|   I am|       so|0.012609652820641122|\n",
      "|   I am|     told|0.010630510091000567|\n",
      "|   I am|    quite|0.009766306653793938|\n",
      "|   I am|convinced|0.008903514929253589|\n",
      "|   I am|       no| 0.00879316605583951|\n",
      "|   I am|       of|0.008537489206280238|\n",
      "|   I am| indebted|0.007908179071046009|\n",
      "+-------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even though some were for many people who had not heard the word the old system ; a few other particulars ; first appeared . A general name applied in practice this would mean ' in his way of doing much more difficult of attainment by the same name with an intention to write a note with regret on quitting it and I know it not a single - handed ; a man were made in the same year his only hope I have often told his wife — I should say . The first two parts . One can only get some good work in connection with the same time the most beautiful girl was in no degree from a of our nature are often not very great and dangerous in themselves so . ' • It is a large scale the rocks under the supervision and the other day ? Is there something that the of thefe things ( p . 5 ) of a of us . I shall write of you ? Do we then have we to know something . Even in her own hands ) I can say — ' The\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "words = ['romeo', 'et']\n",
    "sentence_finished = False\n",
    "while not sentence_finished:\n",
    "    r = random.uniform(0.02,0.001)\n",
    "    if \"'\" in words[-2:]:\n",
    "        a = spark.sql('select * from model where word_12=\"%s%s%s\" and probability>%s'%(words[-2],' ',words[-1],r))\n",
    "    else:\n",
    "        a = spark.sql(\"select * from model where word_12 = '%s%s%s' and probability>%s\" %(words[-2],' ',words[-1],r))        \n",
    "    if a.count() != 0:\n",
    "        words.append(a.rdd.takeSample(False,1)[0][1])\n",
    "    else:\n",
    "        words.append(None)\n",
    "        \n",
    "    if words[-2:]==[None, None]:\n",
    "        sentence_finished=True\n",
    "print(' '.join([t for t in words if t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
